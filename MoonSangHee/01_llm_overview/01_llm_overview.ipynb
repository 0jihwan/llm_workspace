{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNTZjsc3EPiKq2osVWkFnlj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# LLM\n","\n","**모델 파라미터에 따른 LLM구분**\n","\n","LLM(대규모 언어 모델)은 주로 모델 파라미터(매개변수, parameters)의 개수에 따라 모델의 크기와 성능이 구분된다. 파라미터 수가 많을수록 일반적으로 더 복잡하고 다양한 언어 패턴을 학습할 수 있으며, 성능도 향상되는 경향이 있다.\n","\n","* **LLM**: 175B \\~ nT\n","* **SLM**: 6,7B ~ 30B\n","  * 파인튜닝후 일반화 성능이 떨어지는 경우가 많다.\n","  * 특히 10B 밑의 모델들이 더 그렇다*\n","* **On-device AI**: 1~2B\n","\n","**open/close-source 여부에 따른 LLM구분**\n","\n","LLM은 공개 범위와 활용 방식에 따라 크게 **개방형(Open Source, 오픈소스)**과 **폐쇄형(Closed Source, 독점/상용)**으로 구분할 수 있다.\n","\n","| 구분      | 개방형(Open Source) LLM              | 폐쇄형(Closed Source) LLM                 |\n","|-----------|--------------------------------------|-------------------------------------------|\n","| 라이선스   | 오픈소스 라이선스(예: Apache, MIT)    | 벤더(기업) 라이선스, 상용, API 기반         |\n","| 모델 접근 | 소스 코드, 가중치, 아키텍처 모두 공개 | 내부 구조/가중치 비공개, API로만 접근        |\n","| 커스터마이징 | 자유롭게 수정·파인튜닝 가능           | 제한적, 벤더가 허용하는 범위 내에서만 가능   |\n","| 기술 지원 | 커뮤니티 중심, 자발적 기여            | 벤더의 공식 지원, SLA 제공                  |\n","| 보안/프라이버시 | 자체 서버 배포 가능, 유연한 보안 적용  | 벤더 인프라 의존, 데이터 외부 전송 필요       |\n","| 비용 구조  | 인프라 직접 부담, 무료 활용 가능       | API 호출별 과금, 구독 등 상용 모델           |\n","| 대표 예시  | Meta LLaMA, Mistral, GPT-NeoX 등     | OpenAI GPT-3/4, Google Gemini, Anthropic Claude 등 |\n","\n","- **개방형 LLM**은 연구자와 개발자가 자유롭게 모델을 활용·수정할 수 있어 빠른 혁신과 다양한 커스터마이징이 가능하다. 단, 자체 운영·관리 역량이 필요하다.\n","- **폐쇄형 LLM**은 벤더가 모델을 소유·운영하며, 사용자는 API 형태로 접근한다. 보안, 신뢰성, 공식 지원이 강점이나, 내부 구조나 가중치 접근이 제한된다."],"metadata":{"id":"0P9qbE8nyFcD"}},{"cell_type":"markdown","source":["# LLM Vendor별 테스트\n","\n","1. **번역** :\n","\n","   ```text\n","   The dominant sequence transduction models are based on complex recurrent or\n","   convolutional neural networks that include an encoder and a decoder. The best\n","   performing models also connect the encoder and decoder through an attention\n","   mechanism. We propose a new simple network architecture, the Transformer,\n","   based solely on attention mechanisms, dispensing with recurrence and convolutions\n","   entirely. Experiments on two machine translation tasks show these models to\n","   be superior in quality while being more parallelizable and requiring significantly\n","   less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including\n","   ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n","   our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n","   training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n","   best models from the literature. We show that the Transformer generalizes well to\n","   other tasks by applying it successfully to English constituency parsing both with\n","   large and limited training data.\n","\n","   영어로 작성된 과학 저널 논문 초록을 한국어로 번역하되, 전문 용어(예: “photovoltaic efficiency”, “bandgap engineering”)를 정확하게 반영하고, 논문 특유의 딱딱한 문체를 유지하라.\n","   ```\n","   - [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385)\n","   - [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)\n","\n","2. **코드 생성 (난이도 중상)**\n","\n","   ```text\n","   최근 비트코인 가격데이터를 가져와서 시각화 하는 코드를 작성해줘.\n","   - 설치가 필요한 라이브러리가 있는 경우, 설치코드 역시 작성할 것!\n","   ```\n","\n","3. **창의적 글짓기**\n","\n","   ```text\n","   “인간과 로봇이 공존하는 미래 도시”를 배경으로,\n","   200자 내외의 짧은 SF 단편을 한 편 써줘.\n","   ```\n","\n","4. **요약**\n","\n","   ```text\n","   아래 기술 블로그 글을 3문장 이내로 간결하게 요약해줘:\n","   “머신러닝 모델의 과적합(overfitting) 문제를 해결하기 위해 정규화(regularization) 기법이 어떻게 사용되는지, L1/L2 페널티의 차이와 장단점을 사례를 들어 설명한다.”\n","   ```\n","5. **환각**\n","\n","   ```text\n","   현재 대한민국 대통령이 누구야?\n","   강남에 유명한 성형외과 의사 오지명에 대해 알려줘\n","   ```\n","   - knowledge cutoff 이후 이벤트에 대한 질문\n","   "],"metadata":{"id":"6RlLxEyYzJxY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WeQ2n0oDx7_x"},"outputs":[],"source":[]}]}